program: train_rl.py
method: bayes
metric:
  name: eval/success_rate
  goal: maximize
early_terminate:
  type: hyperband
  min_iter: 100000
  eta: 3

parameters:
  # Algorithm
  algorithm:
    values: ['PPO', 'SAC', 'TD3']
  
  # Core hyperparameters
  learning_rate:
    distribution: log_uniform_values
    min: 1e-5
    max: 1e-3
  
  batch_size:
    values: [64, 128, 256, 512]
  
  n_steps:  # PPO only
    values: [1024, 2048, 4096]
  
  # Network architecture (space-separated integers)
  net_arch:
    values:
      - 128 128
      - 256 256
      - 512 512
  
  # Discount factor
  gamma:
    distribution: uniform
    min: 0.95
    max: 0.999
  
  # PPO specific
  gae_lambda:
    distribution: uniform
    min: 0.9
    max: 0.99
  
  clip_range:
    distribution: uniform
    min: 0.1
    max: 0.4
  
  ent_coef:
    distribution: log_uniform_values
    min: 0.00001
    max: 0.1
  
  vf_coef:
    distribution: uniform
    min: 0.1
    max: 1.0
  
  # Environment parameters
  success_threshold:
    value: 0.005  # Fixed at 5mm (0.005m)
  
  max_steps:
    values: [300, 500, 700]
  
  reward_scale:
    distribution: log_uniform_values
    min: 0.5
    max: 5.0
  
  max_velocity:
    distribution: uniform
    min: 0.03
    max: 0.08
  
  # Training
  total_timesteps:
    value: 1000000  # Start with 1M for faster sweeps
  
  n_envs:
    values: [1, 2, 4]
  
  seed:
    values: [42, 123, 456]
